---
layout:     post
title:      决策树
subtitle:   《统计学习方法》笔记 第5章
date:       2020-01-18
author:     Cheereus
header-img: img/post-bg-debug.png
catalog: true
tags:
    - Machine Learning
    - 统计学习方法
---

## 第5章 决策树

### 5.1 决策树模型与学习

#### 5.1.1 决策树模型

决策树(decision tree)是一种基本的分类与回归方法。本章主要讨论用于分类的决策树，其定义为：

* 分类决策树模型是一种描述对实例进行分类的树形结构。决策树由节点(node)和右向边(directed edge)组成。节点由两种类型：内部节点(internal node)和叶节点(leaf node)。内部节点表示一个特征或属性，叶节点表示一个类。

用决策树分类，从根节点开始，对实例的某一特征进行测试，根据测试结果，将实例分配到其子节点；这时，每一个子节点对应着该特征的一个取值。如此递归地对实例进行测试并分配，直至到达叶节点。最后将实例节点分到叶节点的类中。

#### 5.1.2 决策树与 if-then 规则

可以将决策树看成一个 if-then 规则的集合。将决策树转换成 if-then 规则的过程如下：

由决策树的根节点到叶节点的每一条路径构建一条规则；

路径上内部节点的特征对应着规则的条件，而叶节点的类对应着规则的结论。

决策树的路径或其对应的 if-then 规则集合具有一个重要的性质：互斥并且完备。这就是说，每一个实例都被一条路径或一条规则覆盖，而且只被一条路径或一条规则所覆盖。

#### 5.1.3 决策树与条件概率分布

决策树还表示给定特征条件下类的条件概率分布。这一条件概率分布定义在特征空间的一个划分(partition)上。将特征空间划分为互不相交的单元(cell)或区域(region)，并在每个单元定义一个类的概率分布就构成了一个条件概率分布。决策树的一条路经对应于划分中的一个单元。决策树所表示的条件概率分布由各个单元给定条件下类的条件概率分布组成。

#### 5.1.4 决策树学习

假设给定训练数据集

$$D=\{(x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N)\}$$

其中，$x_i=(x^{(1)_i},x^{(2)_i},\cdots,x^{(n)_i})^T$ 为输入实例(特征向量)，$n$ 为特征个数，$y_i \in \\{1,2,\cdots,K\\}$ 为类标记，$i=1,2,\cdots,N$，$N$ 为样本容量。决策树学习的目标是根据给定的训练数据集构建一个决策树模型，使它能够对实例进行正确的分类。

决策树学习本质上是从训练数据集中归纳出一组分类规则。从另一个角度看，决策树学习是由训练数据集估计条件概率模型。基于特征空间划分的类的条件概率模型有无穷多个。我们选择的条件概率模型应该不仅对训练数据有很好的拟合，而且对未知数据有很好的预测。

决策树学习用损失函数表示这一目标。决策树学习的损失函数通常是正则化的极大似然函数。决策树学习的策略是以损失函数为目标函数的最小化。

当损失函数确定以后，学习问题就变为在损失函数意义下选择最优决策树的问题。因为从所有可能的决策树中选取最优决策树是 NP 完全问题，所以现实中决策树学习算法通常采用启发式方法，近似求解这一最优化问题。这样得到的决策树是次最优(sub-optimal)的。

决策树学习算法包含：特征选择、决策树的生成与决策树的剪枝过程。

决策树的生成只考虑局部最优，决策树的剪枝则考虑全局最优。

### 5.2 特征选择

#### 5.2.1 特征选择问题

特征选择在于选取对训练数据具有分类能力的特征，这样可以提高决策树学习的效率。如果利用一个特征进行分类的结果与随机分类的结果没有很大差别，则称这个特征是没有分类能力的，经验上扔掉这样的特征对决策树学习的精度影响不打。

通常特征选择的准则是信息增益或信息增益比。

#### 5.2.2 信息增益

在信息论与概率统计中，熵(entropy)是表示随机变量不确定性的度量。设 $X$ 是一个取有限个值的离散随机变量，其概率分布为：

$$P(X=x_i)=p_i, \ i=1,2,\cdots,n$$

则随机变量 $X$ 的熵定义为：

$$H(X)=-\sum_{i=1}^n p_i \log p_i$$

其中，若 $p_i=0$ 则定义 $0\log 0 = 0$。通常此式中的对数以 $2$ 为底或以 $e$ 为底，这时熵的单位分别称作比特(bit)或者纳特(nat)。由定义可知，熵只依赖于 $X$ 的分布，而与 $X$ 的取值无关，所以也可将 $X$ 的熵记作 $H(p)$，即：

$$H(p)=-\sum_{i=1}^n p_i \log p_i$$

熵越大，随机变量的不确定性就越大。从定义可验证：

$$0 \leq H(p) \leq \log n$$

当随机变量只取两个值，例如 $1, 0$，即 $X$ 的分布为：

$$P(x=1)=p, \ P(X=0)=1-p, \ 0 \leq p \leq 1 $$

熵为：

$$H(p)=-p \log_2p - (1-p)\log_2(1-p)$$