---
layout:     post
title:      朴素贝叶斯法
subtitle:   《统计学习方法》笔记 第4章
date:       2020-01-18
author:     Cheereus
header-img: img/post-bg-debug.png
catalog: true
tags:
    - Machine Learning
    - 统计学习方法
---

## 第4章 朴素贝叶斯法

朴素贝叶斯(naive Bayes)法是基于贝叶斯定理与特征条件独立假设的分类方法。对于给定的数据集，首先基于特征条件独立假设学习输入输出的联合概率分布；然后基于此模型，对给定的输入 $x$，利用贝叶斯定理求出后验概率最大的输出 $y$。朴素贝叶斯法实现简单，学习与预测的效率都很高，是一种常用的方法。

### 4.1 朴素贝叶斯法的学习与分类

#### 4.1.1 基本方法

设输入空间 $\mathcal{X}\subseteq R^n$ 为 $n$ 维向量的集合，输出空间为类标记集合 $\mathcal{Y} = \\{c_1,c_2,\cdots,c_K\\}$。输入为特征向量 $x\in\mathcal{X}$，输出为类标记(class label) $y\in\mathcal{Y}$。$X$ 是定义在输入空间 $\mathcal{X}$ 上的随机变量，$Y$ 是定义在输出空间 $Y$ 上的随机变量。$P(X,Y)$ 是 $X$ 和 $Y$ 的联合概率分布。训练数据集：

$$T=\{(x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N)\}$$

由 $P(X,Y)$ 独立同分布产生。

朴素贝叶斯法通过训练集学习联合概率分布 $P(X,Y)$。具体地，学习以下先验概率分布及条件概率分布。先验概率分布：

$$P(Y=c_k),k=1,2,\cdots,K$$

条件概率分布：

$$P(X=x \vert Y=c_k)=P(X^{(1)}=x^{(1)},\cdots,X^{(n)}=x^{(n)} \vert Y=c_k),k=1,2,\cdots,K$$

于是学习到联合概率分布 $P(X,Y)$。

朴素贝叶斯法对条件概率分布作了条件独立性的假设。由于这是一个较强的假设，朴素贝叶斯法也由此得名。具体地，条件独立性假设是：

$$P(X=x \vert Y=c_k)=P(X^{(1)}=x^{(1)},\cdots,X^{(n)}=x^{(n)} \vert Y=c_k) \\ =\prod^n_{j=1}P(X^{(j)}=x^{(j)} \vert Y=c_k))$$

条件独立性假设等于是说用于分类的特征在类确定的条件下都是条件独立的。这一假设使朴素贝叶斯法变得简单，但有时会牺牲一定的分类准确率。

朴素贝叶斯法分类时，对给定的输入 $x$，通过学习到的模型计算后验概率分布 $P(Y=c_k\vert X=x)$，将后验概率最大的类作为 $x$ 的类输出。后验概率计算根据贝叶斯定理进行：

$$P(Y=c_k\vert X=x)=\frac{P(X=x \vert Y=c_k)P(Y=c_k)}{\displaystyle{\sum_k}{P(X=x \vert Y=c_k)P(Y=c_k)}}$$

根据条件独立性假设有：

$$P(Y=c_k\vert X=x)=\frac{P(Y=c_k) \displaystyle\prod_j P(X^{(j)}=x^{(j)} \vert Y=c_k)}{\displaystyle\sum_k{P(Y=c_k) \displaystyle\prod_j P(X^{(j)}=x^{(j)} \vert Y=c_k)}}$$

这是朴素贝叶斯法分类的基本公式。于是，朴素贝叶斯分类器可以表示为：

$$y=f(x)=\arg\underset{c_k}{\max} \frac{P(Y=c_k) \displaystyle\prod_j P(X^{(j)}=x^{(j)} \vert Y=c_k)}{\displaystyle\sum_k{P(Y=c_k) \displaystyle\prod_j P(X^{(j)}=x^{(j)} \vert Y=c_k)}}$$

注意到，此式中分母对所有的 $c_k$ 都是相同的，所以有：

$$y=f(x)=\arg\underset{c_k}{\max}{{P(Y=c_k) \displaystyle\prod_j P(X^{(j)}=x^{(j)} \vert Y=c_k)}}$$

#### 4.1.2 后验概率最大化的含义



### 第3章完结
