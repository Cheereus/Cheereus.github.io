---
layout:     post
title:      逻辑斯谛回归与最大熵模型
subtitle:   《统计学习方法》笔记 第6章
date:       2020-01-22
author:     Cheereus
header-img: img/post-bg-debug.png
catalog: true
tags:
    - Machine Learning
    - 统计学习方法
---

## 第6章 逻辑斯谛回归与最大熵模型

### 6.1 逻辑斯谛回归模型

#### 6.1.1 逻辑斯谛分布

逻辑斯谛分布(logistic distribution)定义：

设 $X$ 是连续随机变量，$X$ 服从逻辑斯谛分布是指 $X$ 具有下列分布函数和密度函数：

$$F(x)=P(X \leq x)=\frac{1}{1+e^{-(x-\mu)/\gamma} \ }$$

$$f(x)=F'(x)=\frac{e^{-(x-\mu)/\gamma} \ }{\gamma(1+e^{-(x-\mu)/\gamma} \ )^2}$$

其中 $\mu$ 为位置参数，$\gamma > 0$ 为形状参数。

#### 6.1.2 二项逻辑斯谛回归模型

二项逻辑斯谛回归模型(binomial logistic regression model)是一种分类模型，由条件概率分布 $P(Y \vert X)$ 表示，形式为参数化的逻辑斯谛分布：

$$P(Y=1 \vert x)=\frac{\exp(\omega\cdot x+b)}{1+\exp(\omega\cdot x+b)}$$

$$P(Y=0 \vert x)=\frac{1}{1+\exp(\omega\cdot x+b)}$$

其中，$x\in R^n$ 是输入，$Y \in \\{0,1\\}$ 是输出，$\omega\in R^n$ 和 $b \in R$ 是参数，$\omega$ 称为权值向量。$b$ 称为偏置，$\omega\cdot x$ 为 $\omega$ 和 $x$ 的内积。

逻辑斯谛回归比较两个条件概率的大小，将实例 $x$ 分到概率值较大的那一类。

有时为了方便，将权值向量和输入向量加以扩充，扔记作 $\omega,x$，即 $\omega=(\omega^{(1)},\omega^{(2)},\cdots,\omega^{(n)},b)^T, x=(x^{(1)},x^{(2)},\cdots,x^{(n),1})^T$。这时，逻辑斯谛回归模型如下：

$$P(Y=1 \vert x)=\frac{\exp(\omega\cdot x)}{1+\exp(\omega\cdot x)}$$

$$P(Y=0 \vert x)=\frac{1}{1+\exp(\omega\cdot x)}$$

一个事件的几率(odds)是指该事件发生的概率与该事件不发生的概率的比值。如果事件发生的概率是 $p$ 那么该事件的几率是 $\displaystyle\frac{p}{1-p}$，该事件的对数几率(log odds)或 logit 函数是

$$\text{logit}(p)=log\frac{p}{1-p}$$

对逻辑斯谛回归而言，有：

$$\log\frac{P(Y=1 \vert x)}{1-P(Y=1 \vert x)}=\omega\cdot x$$

即在逻辑斯谛回归模型中，输出 $Y=1$ 的对数几率是输入 $x$ 的线性函数。或者说，输出 $Y=1$ 的对数几率是由输入 $x$ 的线性函数表示的模型，即逻辑斯谛回归模型。

#### 6.1.3 模型参数估计

逻辑斯谛回归模型学习时，对于给定的训练数据集 $T=\\{(x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N)\\}$，其中 $x_i \in R_n, \ y_i \in \\{0,1\\}$，可以应用极大似然估计法估计模型参数，从而得到逻辑斯谛回归模型。

设：

$$P(Y=1 \vert x)=\pi(x),\ P(Y=0 \vert x)=1-\pi(x)$$

似然函数为：

$$\prod_{i=1}^N[\pi(x_i)]^{y_i}[1-\pi(x_i)]^{1-y_i}$$

对数似然函数为：

$$L(\omega)=\sum_{i=1}^N[y_i(\omega\cdot x_i)-\log(1+\exp(\omega\cdot x_i))]$$

对 $L(\omega)$ 求极大值，得到 $\omega$ 的估计值。

逻辑斯谛回归通常采用的方法是梯度下降法及拟牛顿法。

#### 6.1.4 多项逻辑斯谛回归

略

### 6.2 最大熵模型

#### 6.2.1 最大熵原理

最大熵原理是概率模型学习的一个准则。最大熵原理认为，学习概率模型时，在所有可能的概率模型(分布)中，熵最大的模型是最好的模型。

假设离散随机变量 $X$ 的概率分布是 $P(X)$，则其熵是

$$H(P)=-\sum_x P(x)\log P(x)$$

熵满足下列不等式：

$$0 \leq H(P) \leq \log\vert X \vert$$

式中，$\vert X \vert$ 是 $X$ 的取值个数，当且仅当 $X$ 的分布是均匀分布时右边的等号成立。也就是说，当 $X$ 服从均匀分布时，熵最大。

最大熵原理通过熵的最大化来表示等可能性。

#### 6.2.2 最大熵模型的定义
