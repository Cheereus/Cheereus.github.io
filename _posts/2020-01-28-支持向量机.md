---
layout:     post
title:      支持向量机
subtitle:   《统计学习方法》笔记 第7章
date:       2020-01-28
author:     Cheereus
header-img: img/post-bg-debug.png
catalog: true
tags:
    - Machine Learning
    - 统计学习方法
---

## 第7章 支持向量机

### 7.1 线性可分支持向量机与硬间隔最大化

#### 7.1.1 线性可分支持向量机

给定线性可分训练数据集，通过间隔最大化或等价地求解相应的凸二次规划问题学习得到的分离超平面为

$$\omega^* \cdot x + b^*=0$$

以及相应的分类决策函数

$$f(x)=\text{sign}(\omega^* \cdot x + b^*)$$

称为线性可分支持向量机。

#### 7.1.2 函数间隔和几何间隔

函数间隔：

对于给定的数据集 $T$ 和超平面 $(\omega,b)$，定义超平面 $(\omega,b)$ 关于样本点 $(x_i,y_i)$ 的函数间隔为

$$\hat{\gamma_i}=y_i(\omega\cdot x+b)$$

定义超平面 $(\omega,b)$ 关于训练数据集 $T$ 的函数间隔为超平面 $(\omega,b)$ 关于 $T$ 中所有样本点 $(x_i,y_i)$ 的函数间隔之最小值，即

$$\hat{\gamma}=\underset{i=1,\cdots,N}{\min}\hat{\gamma_i}$$

几何间隔：

对于给定的训练数据集 $T$ 和超平面 $(\omega,b)$，定义超平面 $(\omega,b)$ 关于样本点 $(x_i,y_i)$ 的几何间隔为

$$\gamma_i=y_i\bigg(\frac{\omega}{\lVert\omega\rVert}\cdot x_i +\frac{b}{\lVert\omega\rVert}\bigg)$$

定义超平面 $(\omega,b)$ 关于训练数据集 $T$ 的几何间隔为超平面 $(\omega,b)$ 关于 $T$ 中所有样本点 $(\omega,b)$ 的几何间隔之最小值，即

$$\gamma=\underset{i=1,\cdots,N}{\min}\gamma_i$$

由上述定义可知，函数间隔和几何间隔有下面的关系：

$$\gamma_i=\frac{\hat\gamma_i}{\lVert\omega\rVert}$$

$$\gamma=\frac{\hat\gamma}{\lVert\omega\rVert}$$

#### 7.1.3 间隔最大化

##### 1. 最大间隔分离超平面

输入：线性可分的数据集 $T=\\{(x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N)\\}$，其中 $x_i \in \mathcal{R}^n,y \in \\{+1,-1\\},i=1,2,\cdots,N$；

输出：最大间隔分离超平面和分类决策函数。

(1) 构造并求解约束最优化问题：

$$\underset{\omega,b}{\min}\frac{1}{2}\lVert\omega\rVert^2$$

$$s.t.\ \ y_i(\omega\cdot x_i+b)-1 \geq 0,\ \ i=1,2,\cdots,N$$

求得最优解 $\omega^*,b^*$。

(2) 由此得到分离超平面：

$$\omega^*\cdot x+b^*=0$$

分类决策函数：

$$f(x)=\text{sign}(\omega^*\cdot x+b^*)$$

##### 2. 最大间隔分离超平面的存在唯一性

若训练数据集 $T$ 线性可分，则可将训练数据集中的样本点完全正确分开的最大间隔分离超平面存在且唯一。

##### 3. 支持向量和间隔边界

在线性可分情况下，训练数据集的样本点中与分离超平面距离最近的样本点的实例称为支持向量(support vector)。支持向量是使约束条件式等号成立的点，即

$$y_i(\omega\cdot x_i+b)-1=0$$

对 $y_i=+1$ 的正例点，支持向量在超平面

$$H_1:\omega\cdot x+b=1$$

上。对 $y_i=-1$ 的正例点，支持向量在超平面

$$H_2:\omega\cdot x+b=-1$$

上。在 $H_1$ 和 $H_2$ 上的点就是支持向量。$H_1$ 和 $H_2$ 之间的距离称为间隔(margin)。间隔依赖于分离超平面的法向量 $\omega$，等于$\frac{2}{\lVert\omega\rVert}$。$H_1$ 和 $H_2$ 称为间隔边界。

在决定分离超平面时只有支持向量起作用，而其他实例点不起作用。由于支持向量在确定分离超平面中起着决定性作用，所以将这种分类模型称为支持向量机。

#### 7.1.4 学习的对偶算法

为了求解线性可分支持向量机的最优化问题，将它作为原始最优化问题，应用拉格朗日对偶性，通过求解对偶问题(dual problem)得到原始问题(primal problem)的最优解，这就是线性可分支持向量机的对偶算法(dual algorithm)。

##### 线性可分支持向量机学习算法

输入：线性可分的数据集 $T=\\{(x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N)\\}$，其中 $x_i \in \mathcal{R}^n,y \in \\{+1,-1\\},i=1,2,\cdots,N$；

输出：分离超平面和分类决策函数

(1) 构造并求解约束最优化问题

$$\underset{\alpha}{\min}\ \frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jy_iy_j(x_i\cdot x_j)-\sum_{i=1}^N\alpha_i$$

$$s.t.\ \ \sum_{i=1}^Na_iy_i=0$$

$$a_i\geq 0, \ i=1,2,\cdots,N$$

求得最优解 $\alpha^*=(\alpha^*,\alpha^*,\cdots,\alpha^*_N)^T$。

(2) 计算

$$\omega^*=\sum_{i=1}^N\alpha^*_iy_ix_i$$

并选择 $\alpha^*$ 的一个正分量 $\alpha^*_j>0$，计算

$$b^*=y_i-\sum_{i=1}^N\alpha_i^*y_i(x_i\cdot x_j)$$

(3) 求得分离超平面

$$\omega^*\cdot x+b^*=0$$

分类决策函数：

$$f(x)=\text{sign}(\omega^*\cdot x+b^*)$$

在线性可分支持向量机中，$\omega^*$ 和 $b^*$ 只依赖于训练数据中对应于 $\alpha^*_i>0$ 的样本点 $(x_i,y_i)$，而其他样本点对 $\omega^*$ 和 $b^*$ 没有影响。我们给训练数据中对应于 $\alpha^*_i>0$ 的实例点 $x_i\in R^n$ 称为支持向量。

### 7.2 线性支持向量机与软间隔最大化

#### 7.2.1 线性支持向量机

对于给定的线性不可分的训练数据集，通过求解凸二次规划问题，即软间隔最大化问题，得到的分离超平面为

$$\omega^* \cdot x + b^*=0$$

以及相应的分类决策函数

$$f(x)=\text{sign}(\omega^* \cdot x + b^*)$$

称为线性支持向量机。

#### 7.2.2 学习的对偶算法

输入：线性可分的数据集 $T=\\{(x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N)\\}$，其中 $x_i \in \mathcal{R}^n,y \in \\{+1,-1\\},i=1,2,\cdots,N$；

输出：分离超平面和分类决策函数。

(1) 选择惩罚函数 $C>0$，构造并求解凸二次规划问题

$$\underset{\alpha}{\min}\ \frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jy_iy_j(x_i\cdot x_j)-\sum_{i=1}^N\alpha_i$$

$$s.t.\ \ \sum_{i=1}^Na_iy_i=0$$

$$0 \leq a_i \leq C, \ i=1,2,\cdots,N$$

求得最优解 $\alpha^*=(\alpha^*,\alpha^*,\cdots,\alpha^*_N)^T$。

(2) 计算 $\omega^*=\displaystyle\sum_{i=1}^N\alpha^*_iy_ix_i$

选择 $\alpha^*$ 的一个分量 $\alpha_j^*$ 适合条件 $0<\alpha^*_j<C$，计算

$$b^*=y_i-\sum_{i=1}^N\alpha_i^*y_i(x_i\cdot x_j)$$

(3) 求得分离超平面

$$\omega^*\cdot x+b^*=0$$

分类决策函数：

$$f(x)=\text{sign}(\omega^*\cdot x+b^*)$$

#### 7.2.3 支持向量

见原书附图

#### 7.2.4 合页损失函数
